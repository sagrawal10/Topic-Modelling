{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this notebook we discuss LDA algorithm and \n",
    "how it can be used to identify topics within documents. \n",
    "In particular, we apply the LDA algorithm for topic analysis \n",
    "on a youtube spam classification comments data set and \n",
    "find that LDA discovered the topic 1 \"spam\" and \n",
    "topic 2 \"legitimate\" on its own without being provided labels. \n",
    "This is because we noticed that comments that were over 90% topic 1 \n",
    "seemed to be spam, and comments that were over 90% topic 2 \n",
    "seemed to be leigitimate. However the first 10 commonly appearing words \n",
    "in both the topics seem to over lap, this may be improved by applying \n",
    "TF-IDF in which frequently appearing words will receive smaller weight.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For matrix operations\n",
    "import numpy as np\n",
    "# For data processing\n",
    "import pandas as pd\n",
    "# For text processing\n",
    "import nltk \n",
    "# For regular expressions\n",
    "import re\n",
    "# For dividing numbers\n",
    "#from __future__ import division\n",
    "\n",
    "# Set seed so we get same random allocation on each run of code\n",
    "np.random.seed(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text data\n",
    "comments = pd.read_csv(\"YoutubeCommentsSpam.csv\")\n",
    "\n",
    "# Let's take a look at the first few rows\n",
    "print comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer function\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Tokenizing first observation\n",
    "tokenize_obs = tokenizer.tokenize(comments[\"commentText\"][1])\n",
    "\n",
    "# Example of tokenizing first observation\n",
    "print('Tokenize first observation: \\n%s' % tokenize_obs)\n",
    "\n",
    "# Load list of common stop words\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "# Create English stop words list\n",
    "eng_stop = [str(word) for word in get_stop_words('english')]\n",
    "\n",
    "# Print a few stop words\n",
    "print('Stop words in english: \\n%s' % eng_stop[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function for stemming text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "# Text data to itterate over\n",
    "text_data = [line for line in comments[\"commentText\"] if line != '']\n",
    "\n",
    "# Convert text data into a list of comments after stop words and stemming are accounted for\n",
    "for line in range(len(comments)):\n",
    "    \n",
    "    # Convert comment all to lower case\n",
    "    raw_lower = text_data[line].lower()\n",
    "    \n",
    "    # Tokenize comment\n",
    "    line_token = tokenizer.tokenize(raw_lower)\n",
    "    \n",
    "    # Only keep letters in comments\n",
    "    clean_token = [re.sub(r'[^a-zA-Z]','', word) for word in line_token]\n",
    "    \n",
    "    # Take out stop words\n",
    "    stop_token = [word for word in clean_token if not word in eng_stop if word != '']\n",
    "    \n",
    "    # Take out stem words\n",
    "    stem_token = [str(p_stemmer.stem(word)) for word in stop_token]\n",
    "    \n",
    "    # Replace comment with cleaned list of words\n",
    "    text_data[line] = stop_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to convert our list of list data into single list\n",
    "#  if words != ''\n",
    "words_list = [words for sublist in text_data for words in sublist]\n",
    "\n",
    "# Vocabulary is the set of unique words used\n",
    "vocab_total = set(words_list)\n",
    "\n",
    "# Take a look at few words\n",
    "print('Few words from vocabulary list: \\n%s' % list(vocab_total)[1:7])\n",
    "\n",
    "# Size of vocabulary list \n",
    "print('Number of unique words in data: \\n%s' % len(vocab_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each comment into a vector by replacing the words by their unique ID\n",
    "text_ID = []\n",
    "\n",
    "# Loop over cleaned text data\n",
    "for line in range(len(text_data)):\n",
    "    \n",
    "    # Append comment replaced by unique word IDs\n",
    "    comment_vector = [list(vocab_total).index(words) for words in text_data[line]]\n",
    "    text_ID.append(comment_vector)\n",
    "\n",
    "# Let's check the first comment\n",
    "print (\"The first comment (after processing) is: \\n%s\" % text_data[0])\n",
    "print('First comment as a vector of word IDs is: \\n%s' % text_ID[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hyperparameters in LDA\n",
    "\n",
    "# Dirichlet parameters\n",
    "# Alpha is the parameter for the prior topic distribution within documents\n",
    "alpha = 0.2\n",
    "\n",
    "# Beta is the parameter for the prior topic distribution within documents\n",
    "beta = 0.001\n",
    "\n",
    "# Text corpus itterations\n",
    "corpus_itter = 200\n",
    "\n",
    "# Number of topics\n",
    "K = 2\n",
    "\n",
    "# Vocabulary size\n",
    "V = len(vocab_total)\n",
    "\n",
    "# Number of Documents\n",
    "D = len(text_ID)\n",
    "\n",
    "# For practical implementation, we will generate the following three count matrices:\n",
    "# 1) Word-Topic count matrix, 2) Topic-Document assignment matrix, 3) Document-Topic count matrix\n",
    "\n",
    "# Initialize word-topic count matrix (size K x V, K = # topics, V = # vocabulary)\n",
    "word_topic_count = np.zeros((K,V))\n",
    "\n",
    "# Initialize topic-document assignment matrix\n",
    "topic_doc_assign = [np.zeros(len(sublist)) for sublist in text_ID] \n",
    "\n",
    "# Initialize document-topic matrix\n",
    "doc_topic_count = np.zeros((D,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word-topic count matrix with randomly assigned topics\n",
    "\n",
    "# Loop over documents\n",
    "for doc in range(D):\n",
    "    \n",
    "    # Loop over words in given document\n",
    "    for word in range(len(text_ID[doc])):\n",
    "\n",
    "        # Step 1: Randomly assign topics to each word in document\n",
    "        # Note random.choice generates number {0,...,K-1}\n",
    "        topic_doc_assign[doc][word] = np.random.choice(K,1)\n",
    "\n",
    "        # Record word-topic and word-ID\n",
    "        word_topic = int(topic_doc_assign[doc][word])\n",
    "        word_doc_ID = text_ID[doc][word]\n",
    "        \n",
    "        # Increment word-topic count matrix\n",
    "        word_topic_count[word_topic][word_doc_ID] += 1\n",
    "\n",
    "# Print word-topic matrix\n",
    "print('Word-topic count matrix with random topic assignment: \\n%s' % word_topic_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate document-topic count matrix with randomly assigned topics\n",
    "\n",
    "# Loop over documents (D = numb. docs)\n",
    "for doc in range(D):\n",
    "    \n",
    "    # Loop over topics (K = numb. topics)\n",
    "    for topic in range(K):\n",
    "        \n",
    "        # topic-document vector\n",
    "        topic_doc_vector = topic_doc_assign[doc]\n",
    "        \n",
    "        # Update document-topic count\n",
    "        doc_topic_count[doc][topic] = sum(topic_doc_vector == topic)\n",
    "\n",
    "# Print document-topic matrix\n",
    "print('Subset of document-topic count matrix with random topic assignment: \\n%s' % doc_topic_count[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main part of LDA algorithm (takes a few minutes to run)\n",
    "# Run through text corpus multiple times\n",
    "for itter in range(corpus_itter):\n",
    "    \n",
    "    # Loop over all documents\n",
    "    for doc in range(D):\n",
    "        \n",
    "        # Loop over words in given document\n",
    "        for word in range(len(text_ID[doc])):\n",
    "            \n",
    "            # Initial topic-word assignment\n",
    "            init_topic_assign = int(topic_doc_assign[doc][word])\n",
    "            \n",
    "            # Initial word ID of word \n",
    "            word_id = text_ID[doc][word]\n",
    "            \n",
    "            # Before finiding posterior probabilities, remove current word from count matrixes\n",
    "            doc_topic_count[doc][init_topic_assign] -= 1\n",
    "            word_topic_count[init_topic_assign][word_id] -=1\n",
    "            \n",
    "            # Find probability used for reassigning topics to words within documents\n",
    "            \n",
    "            # Denominator in first term (Numb. of words in doc + numb. topics * alpha)\n",
    "            denom1 = sum(doc_topic_count[doc]) + K*alpha\n",
    "            \n",
    "            # Denominator in second term (Numb. of words in topic + numb. words in vocab * beta)\n",
    "            denom2 = np.sum(word_topic_count, axis = 1) + V*beta\n",
    "            \n",
    "            # Numerators, number of words assigned to a topic + prior dirichlet param\n",
    "            numerator1 = [doc_topic_count[doc][col] for col in range(K)] \n",
    "            numerator1 = np.array(numerator1) + alpha\n",
    "            numerator2 = [word_topic_count[row][word_id] for row in range(K)]\n",
    "            numerator2 = np.array(numerator2) + beta\n",
    "            \n",
    "            # Compute conditional probability of assigning each topic\n",
    "            # Recall that this is obtained from gibbs sampling\n",
    "            prob_topics = (numerator1/denom1)*(numerator2/denom2)\n",
    "            prob_topics = prob_topics/sum(prob_topics)\n",
    "                                    \n",
    "            # Update topic assignment (topic can be drawn with prob. found above)\n",
    "            update_topic_assign = np.random.choice(K,1,list(prob_topics))\n",
    "            topic_doc_assign[doc][word] = update_topic_assign\n",
    "            \n",
    "            # Add in current word back into count matrixes\n",
    "            doc_topic_count[doc][init_topic_assign] += 1\n",
    "            word_topic_count[init_topic_assign][word_id] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posterior mean of document-topic distribution\n",
    "theta = (doc_topic_count+alpha)\n",
    "theta_row_sum = np.sum(theta, axis = 1)\n",
    "theta = theta/theta_row_sum.reshape((D,1))\n",
    "\n",
    "# Print document-topic mixture\n",
    "print('Subset of document-topic mixture matrix: \\n%s' % theta[0:3])\n",
    "\n",
    "# Spam comment\n",
    "print ('Comment is 95 perc. topic 1, and 5 perc. topic 2: \\n%s' % theta[10])\n",
    "print ('Comment looks like its spam: \\n%s' % comments[\"commentText\"][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spam comment\n",
    "print ('Comment is 92 perc. topic 1, and 8 perc. topic 2: \\n%s' % theta[4])\n",
    "print ('Comment seems to be spam: \\n%s' % comments[\"commentText\"][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-spam comment\n",
    "print ('Comment is 8 perc. topic 1 and 92 perc. topic 2: \\n%s' % theta[11])\n",
    "print ('Comment seems ligitimate: \\n%s' % comments[\"commentText\"][11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-spam comment\n",
    "print ('Comment is is 5 perc. topic 1 and 95 perc. topic 2: \\n%s' % theta[18])\n",
    "print ('Comment seems to be about video, non-spam: \\n%s' % comments[\"commentText\"][18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posterior mean of word-topic distribution within documents\n",
    "phi = (word_topic_count + beta)\n",
    "phi_row_sum = np.sum(phi, axis = 1)\n",
    "phi = phi/phi_row_sum.reshape((K,1))\n",
    "\n",
    "\n",
    "# Print topic-word mixture\n",
    "print('Topic-word mixture matrix: \\n%s' % phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the top words that make up each topic \n",
    "\n",
    "# Initialize list of dictionaries\n",
    "list_dict_topics = []\n",
    "\n",
    "# Loop over topics\n",
    "for topic in range(K):\n",
    "    \n",
    "    # Initialize (vocab,prob) dictionary\n",
    "    mydict = {}\n",
    "    \n",
    "    # Loop over vocabular\n",
    "    for word in range(V):\n",
    "        \n",
    "        # Create dictionary {(vocab,prob)}\n",
    "        mydict[list(vocab_total)[word]] = phi[topic][word]\n",
    "        \n",
    "    # Create list of dictionaries\n",
    "    list_dict_topics.append(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First topic\n",
    "# The first 10 words are ignored, because they most overlap with topic 2\n",
    "# Commonly appearing words in topic 1\n",
    "sorted([(value,key) for (key,value) in list_dict_topics[0].items()])[::-1][10:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second topic\n",
    "# The first 10 words are ignored, because they most overlap with topic 1\n",
    "# Commonly appearing words in topic 2\n",
    "sorted([(value,key) for (key,value) in list_dict_topics[1].items()])[::-1][10:30]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
